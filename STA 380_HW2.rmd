---
title: 'STA 380 Homework2: Li_Peng_Wang'
author: "Daniel Peng, Anying Li, Jiaqiu Wang"
date: "August 15th, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Q1: Flights at ABIA

Create a figure, or set of related figures, that tell an interesting story about flights into and out of Austin. You can annotate the figure and briefly describe it, but strive to make it as stand-alone as possible. It shouldn't need many, many paragraphs to convey its meaning. Rather, the figure should speak for itself as far as possible. For example, you might consider one of the following questions:

What is the best time of day to fly to minimize delays?
What is the best time of year to fly to minimize delays?
How do patterns of flights to different destinations or parts of the country change over the course of the year?
What are the bad airports to fly to?

But anything interesting will fly.

```{r}
library(ggplot2)
library(plyr)
library(tidyr)
library(lubridate)
library(dplyr)

ABIA = read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv",header=T)

```
read in the files and load in related library.

## what is the best month to fly to minimize delay?
```{r}
flight.count = ddply(ABIA, ~Month, summarise, total_flight = length(FlightNum))

delayed = ABIA[which(ABIA$DepDelay >= 0),]
delayed.count = ddply(delayed, ~Month, summarise, delayed_flight = length(FlightNum))

delay.chance = delayed.count$delayed_flight /flight.count$total_flight

par(mar=c(5,4,2,3)+.1)
plot(delay.chance, xlab = "Months", ylab = 'Probability', type = 'b', main = 'Chance of Departure Delay')
```

After having a few scatterplot, we found that there are lots of outliers for the delay data, and we cannot simply use mean as a measure for delay. We decide to look at the probability of delay--although this cannot reflect the length of the delay, this can be compensated by our later analysis.

According to the plot, flights departuring in September to November have the lowest chance of being delayed while the flights in December have the highest. One explaination to this phenomenon is that people tend to travel less before the holiday season to save vacation days. Thus the airport is less busy. So This is the best time to fly from Austin if one is time sensitive.

## what is the best day of a week to fly to minimize delay?

```{r}
flight.count_2 = ddply(ABIA, ~DayOfWeek, summarise, total_flight = length(FlightNum))
delayed.count_2 = ddply(delayed, ~DayOfWeek, summarise, delayed_flight = length(FlightNum))
delay.chance_2 = delayed.count_2$delayed_flight /flight.count_2$total_flight

par(mar=c(5,4,2,3)+.1)
plot(delay.chance_2, xlab = "Day of Week", ylab = 'Probability', type = 'b', main = 'Chance of Departure Delay')
```

The probability of delay peaks on Friday and dips on Saturday. There is also a pickup on Sunday and Monday. This is because many business people travel to work on Sundays or Mondays, and go home on Fridays. Many people also choose to fly on Fridays and go home on Sundays for weekend trips.

## what cause the delay?

```{r}
a = ABIA[,c('Month','WeatherDelay','NASDelay','CarrierDelay','SecurityDelay','LateAircraftDelay')]
a[is.na(a)] <- 0

delay.reason = ddply(a,.(Month),summarise,carrier =sum(CarrierDelay), weather=sum(WeatherDelay), nas=sum(NASDelay), security=sum(SecurityDelay), LateAircraft=sum(LateAircraftDelay))

delay.reason = t(delay.reason)

par(mar=c(4,4,3,4))
barplot(as.matrix(delay.reason)[2:6,], col = c('blue', 'yellow', 'grey', 'brown', 'cyan'), main = "Minutes in Delay and Number of Flights by Months", names.arg = delay.reason[1,], ylab = 'Minutes in Delays', ylim = c(0,200000), legend = rownames(delay.reason)[2:6])

par(new = T)
with(flight.count, plot(flight.count$Month, flight.count$total_flight, type = "l", axes=F, xlab='Months', ylab=NA, cex=.8, lwd = 3, ylim = c(7000,10000)))
axis(side = 4)
mtext(side = 4, line = 3, 'Number of Flights')
```

Late Aircraft, carrier and NAS are the main reasons for delays. The trend in total delay minutes follows the trend in the number of flights throughout the year, with the exception of December. In Decenmber, the airport experiences very high delay time despite low flight volume. Majority of the delays is caused by late aircrafts. This can be caused by many reasons, such as severe weather at the origin or staff shortage at the previous airport since it's the holiday season, etc.

## when is the best time to fly during a day to minimize delay?
```{r}
#departure delays as fun of scheduled departure time
ABIA$CRSDepTime = round(ABIA$CRSDepTime/100,0)

plot_data = ABIA %>%
  gather(DelayType,newdelay,DepDelay) %>%
  group_by(CRSDepTime) %>%
  dplyr::summarise(mu=mean(newdelay,na.rm=TRUE),
                   se=sqrt(var(newdelay,na.rm=TRUE)/length(na.omit(newdelay))),
                   obs=length(na.omit(newdelay)))

p=ggplot(plot_data,aes(x=CRSDepTime,y=mu,min=mu-se,max=mu+se)) +
  geom_line() +
  geom_point() +
  geom_errorbar(width=.33) +
  scale_x_continuous(breaks=seq(6,23)) +
  labs(x="Hour of Day",y="Average Delay (Minutes)",title="Flight Delays by Scheduled Departure Time") +
  theme(legend.position="bottom")
p

#Just the 95% and 75% quantiles
plot_data = ABIA %>%
  group_by(CRSDepTime) %>%
  dplyr::summarise(Quantile_95=quantile(DepDelay,.95,na.rm=TRUE),
                   Quantile_75=quantile(DepDelay,.75,na.rm=TRUE),
                   obs=length(na.omit(DepDelay))) 
plot_data2 = plot_data %>% 
  gather(variable, value, Quantile_75:Quantile_95) %>%
  mutate(variable=factor(variable,levels=c("Quantile_95","Quantile_75")))

p=ggplot(plot_data2,aes(x=CRSDepTime,y=value,group=variable,color=variable)) +
  geom_line() +
  scale_x_continuous(breaks=seq(5,23)) +
  labs(x="Hour of Day",y="Departure Delay (Minutes)",title="95th and 75th Percentiles of Departure Delays")  +
  scale_color_discrete(name="Quantile") +
  theme(legend.position="bottom") 
p
```

From the plot we can see that the later you leave, the greater the average delay you will face. Planes even leave earlier than schedule departure time when the airport just begin to operate in the early morning. It makes sense that delays increase as the day goes on, as we showed in the earlier graph, the primary cause of delays is waiting for the plane to arrive from another airport. The first flights out in the morning donâ€™t have this problem.

By looking at the 75% and 95% percentile chart, we can get an idea that if one's flight is scheduled at night after 7pm, there is a 25% chance that the plane will be delayed for about 15-20 minutes, and there is 5% chance that the plane will be delayed over 90 minutes.

# Q2 Author attribution

## Model 1: TF-IDF with Principal Component Analysis

Set up the environment:

```{r}
rm(list=ls())
library(tm)
```

Read the training data:

```{r}
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

author_dirs = Sys.glob('/Users/leeanthea/STA380-master/data/ReutersC50/C50train/*')
file_list = NULL
labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=57)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

my_corpus = Corpus(VectorSource(all_docs))
names(my_corpus) = file_list
my_corpus = tm_map(my_corpus, content_transformer(tolower)) 
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) 
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation))
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) 
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

unique_author  = unique(labels)
```

Calculate the TFIDF matrix of the training dataset:

```{r}
idf.weight <- function(x) {
  doc.freq <- colSums(x>0)
  doc.freq[doc.freq == 0] <- 1
  w <- log(nrow(x)/doc.freq)
  return(scale.cols(x,w))
}

scale.cols <- function(x,s) {
  return(t(apply(x,1,function(x){x*s})))
}

my_cosine = function(v1, v2) {
  result= NULL
  for(u in (1:50)){
    result[u] = sum(v1 %*% v2[,u]) / {sqrt(sum(v1^2)) * sqrt(sum(v2[,u]^2))}
  }
  return(result)
}

DTM = DocumentTermMatrix(my_corpus)
DTM = removeSparseTerms(DTM, 0.99)
X = as.matrix(DTM)
row.names(X)=labels
DTM_TF = X / rowSums(X)
DTM_TFIDF = idf.weight(DTM_TF)
```

Read the testing data:

```{r}
test_dirs = Sys.glob('/Users/leeanthea/STA380-master/data/ReutersC50/C50test/*')
test_list = NULL
labels_test = NULL
for(author in test_dirs) {
  author_name = substring(author, first=56)
  test_to_add = Sys.glob(paste0(author, '/*.txt'))
  test_list = append(test_list, test_to_add)
  labels_test = append(labels_test, rep(author_name, length(test_to_add)))
}


test_docs = lapply(test_list, readerPlain) 
names(test_docs) = test_list
names(test_docs) = sub('.txt', '', names(test_docs))

test_corpus = Corpus(VectorSource(test_docs))
names(test_corpus) = test_list

test_corpus = tm_map(test_corpus, content_transformer(tolower)) 
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers))
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation)) 
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace)) 
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))
```

Comparing the doc-term matrices of train set and test set, we can see that some words in the test set were never seen in the training set.

```{r}
DTM_test = DocumentTermMatrix(test_corpus)
DTM_test = removeSparseTerms(DTM_test, 0.99)
DTM_test
DTM
```

Select the intersect of train set and test set (words contained in both sets) and calculate the TFIDF matrix of the test set using only these words

```{r}
X_test = DTM_test[,c(intersect(colnames(DTM_TFIDF), colnames(DTM_test)))]
X_test = as.matrix(X_test)
row.names(X_test)=labels_test
DTM_TF_test = X_test / rowSums(X_test)
DTM_TFIDF_test = idf.weight(DTM_TF_test)
```

Run principal component analysis on the train set to find out the latent features of documents

```{r}
lsi = prcomp(DTM_TFIDF[,intersect(colnames(X_test), colnames(DTM_TFIDF))], scale.=FALSE)
```

Construct a query matrix with 1000 principal components and aggregate the loadings of articles written by the same author

```{r}
query_vec = t(lsi$x[,1:1000])
colnames(query_vec)=labels
query_vec = t(rowsum(t(query_vec), group = rownames(t(query_vec))))
```

Project the TFIDF matrix of test data on the 1000 dimensions (principal components) 

```{r}
trans = DTM_TFIDF_test %*% lsi$rotation[,1:1000]
```

Find out the query vector that produces the largest inner product (cosine) with one specific document vector. Attribute the document to the author who corresponds to that query vector.

```{r}
count = 0
predict <- list()
for(i in (1:2500)){
  temp = my_cosine(trans[i,],query_vec)
  predict[[i]] = unique_author[which.max(temp)]
  if(predict[[i]]==labels[i]){
    count = count+1
  }
}
predict = do.call(rbind, predict)
row.names(predict) = labels_test
head(predict)
```

Authors whose articles seem difficult to distinguish from one another are as follow:

```{r}
most_frequent = list()
for(i in (1:50)){
  begin=(i-1)*50+1
  end = i*50
  temp = table(predict[begin:end])
  most_frequent[[i]] = names(which.max(temp))
  if(most_frequent[[i]]!=unique_author[i]){
    print(paste0('author is ',unique_author[i],', model predicts ',most_frequent[[i]]))
  }
}
most_frequent = do.call(rbind, most_frequent)
row.names(most_frequent) = unique_author
most_frequent
```

The accuracy of this model is:

```{r}
accuracy_tfidf = count/nrow(X_test)
accuracy_tfidf
```

## Model 2: Naive Bayes

Construct the multinomial probability matrix of the train set

```{r}
prob <- list()
for(i in (1:50)){
  begin=(i-1)*50+1
  end = i*50
  w = colSums(X[begin:end,])
  prob[[i]] = w
}
prob = do.call(rbind, prob)
row.names(prob) <- unique_author
```

Reshape the multinomial probability matrix of the train set and add a smooth count for unseen words

```{r}
smooth_count = 1/nrow(X)
prob = prob[,intersect(colnames(DTM), colnames(DTM_test))]
for(i in setdiff(colnames(DTM_test), colnames(DTM))){
  prob = cbind(prob,i=smooth_count)
}
prob = prob/rowSums(prob)
prob.T = t(prob)
P = as.matrix(prob.T)
```

The predictions and accuracy of the Naive Bayes model are as follow:

```{r}
count = 0 
predict <- list()
X_test = DTM_test[,c(intersect(colnames(DTM), colnames(DTM_test)),setdiff(colnames(DTM_test),colnames(DTM)))]
X_test = as.matrix(X_test)
for(i in (1:2500)){
  temp = X_test[i,] %*% P
  temp = data.frame(temp)
  predict[[i]] = names(which.max(temp))
  if(predict[[i]]==labels[i]){
    count = count+1
  }
}
predict = do.call(rbind, predict)
row.names(predict) = labels_test
head(predict)
accuracy_naive_bayes = count/nrow(X_test)
accuracy_naive_bayes
```

Authors whose articles seem difficult to distinguish from one another are as follow:

```{r}
most_frequent = list()
for(i in (1:50)){
  begin=(i-1)*50+1
  end = i*50
  temp = table(predict[begin:end])
  most_frequent[[i]] = names(which.max(temp))
  if(most_frequent[[i]]!=unique_author[i]){
    print(paste0('author is ',unique_author[i],', model predicts ',most_frequent[[i]]))
  }
}
```

With the help of PCA, TFIDF model gets an accuracy rate slightly higher than Naive Bayes. More importantly, PCA significantly reduced the running time required for the former model. The TFIDF model is preferred considering the accuracy and efficiency aspects of the two models.


# Q3: Practice with association rule mining

Revisit the notes on association rule mining, and walk through the R example on music playlists: playlists.R and playlists.csv. Then use the data on grocery purchases in groceries.txt and find some interesting association rules for these shopping baskets. The data file is a list of baskets: one row per basket, with multiple items per row separated by commas -- you'll have to cobble together a few utilities for processing this into the format expected by the "arules" package. Pick your own thresholds for lift and confidence; just be clear what these thresholds are and how you picked them. Do your discovered item sets make sense? Present your discoveries in an interesting and concise way.

```{r}
num_col <- max(count.fields("https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt", sep = ","))
raw <- read.table("https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt",sep=",",fill=TRUE,col.names=1:num_col,na.strings=c("","NA"))
raw <- cbind(id = rownames(raw), raw)
dim(raw)

library(reshape)
raw = cbind(raw[1], stack(lapply(raw[2:33], as.character)))
raw = raw[order(raw$id),]
raw = na.omit(raw)
raw = raw[,1:2]

library(arules)
str(raw)
raw$id <- factor(raw$id)
post <- split(x=raw$values, f=raw$id)
post <- lapply(post, unique)
trans <- as(post, "transactions")
rules <- apriori(trans, 
                 parameter=list(support=.001, confidence=.2, maxlen=4))
arules::inspect(subset(rules,subset=lift > 10))
arules::inspect(subset(rules,subset=confidence > 0.8))
arules::inspect(subset(rules,subset=lift > 8 & confidence > 0.6))

```

Here we choose to set the threshold at support level equal to 0.001 and confidence level equal to 0.2. The general rule for choosing threshhold is that the bigger the dataset is, the looser the threshold is. Here the dataset include approximately 10000 transaction records, we choose a relatively low support level because we don't want some of the interesting associations to be missed out just because the co-occurance of two items among the whole dataset is not high enough. And we set the confidence level at 0.2, because a low support and high confidence level help us extract strong relationship even for less overall co-occurrences in data. We also set maxlen = 4, because grocery shopping involves a large amount of randomness, we don't want the correlation to be over-analyzed and just want to focus on the most important/relavant associations.

After we get the rules, we take a closer look at it by first filtering out rules with a high lift (lift >10). First interesting discover is that when people buy sofener, they are 10 times more likely to purchase detergent as well. Second interesting thing is that we find there is a group of people that likes instant food products, and even when they buy meat they prefer to buy hamburger meat, the "quick meat". The third interesting thing is that when people buy alcoholic drinks, they tent to buy different alcoholic drinks together. Perhaps most of people buy alcoholic drinks for party/large event and want to provide different choice for their guests.The forth interesting thing is that we find some "sandwich maker" who like to purchase ham and processed cheese with white bread. We also find the "baker group", and discovered that they tend to but several baking materials at one time. 

Then we tried to look at the association rules that have a high confidence level(confidence > 0.8). Here the result is more close to our common knowledge--when people buy fruit/vegetable/meat, they are more likely to buy whole milk and other vegetables. Most housewives go to grocery shoppings for milk, bread, vegetables and fruit which most family consume most quickly. 

